Tensor networks have been used as theoretical frameworks for understanding deep learning architectures [2, 3], as well as practical tools for compressing large neural networks via "tensorization" [novikov2015]. But tensor networks on their own can serve as expressive models for many machine learning tasks [stoudenmire2016, novikov2016, stoudenmire2018], including unsupervised generative modeling [han2018]. Although general tensor networks involve large computational overhead, matrix product states (MPS) represent efficient tensor network models which are well-adapted for sequential data, with close ties to weighted finite automata [4].

Although MPS generative models have powerful capabilities not available in neural network approaches [ferris2012], their expressive power is limited by area laws upper-bounding the mutual information between disjoint regions of their output sequences [eisert2010]. Here we propose a novel MPS architecture which circumvents this limitation through the use of a tensorized hidden state space. Our model can be seen as a particular form of second-order recurrent neural network [rabusseau2019], but one possessing a hidden state space that grows exponentially in the allocated computational resources. Although still in its early development, we view this architecture as a novel merger of theoretical simplicity with significant expressive power, whose applications to language modeling are a subject of active investigation.
